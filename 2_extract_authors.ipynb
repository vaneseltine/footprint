{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3ac8d6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import httpx\n",
    "\n",
    "# for i in range(10):\n",
    "#     clear_output(wait=True)\n",
    "#     print(i)\n",
    "#     time.sleep(0.1)\n",
    "\n",
    "DATA_DIR = Path('./data')\n",
    "WORKS_DATA_DIR = DATA_DIR / 'works_of_institutions'\n",
    "AUTHORS_DIR = DATA_DIR / 'works_by_author'\n",
    "print(len(list(WORKS_DATA_DIR.glob('*.json'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0bc32",
   "metadata": {},
   "source": [
    "## institutional works to author_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f8a62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openalex.org/W3087139986\n",
      "https://openalex.org/A2814149936 Yuzi Tian ['University of Michigan–Ann Arbor', 'Department of Rheumatology and Immunology, Xiangya Hospital, Central South University, Changsha, Hunan, China.']\n",
      "https://openalex.org/A2607418774 Qiufang Deng ['University of Michigan–Ann Arbor', 'Department of Rheumatology and Immunology, Xiangya Hospital, Central South University, Changsha, Hunan, China.']\n",
      "https://openalex.org/A2274029971 Hasan B. Alam ['University of Michigan–Ann Arbor']\n",
      "https://openalex.org/A2107458222 Yongqing Li ['University of Michigan–Ann Arbor']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "work_file = WORKS_DATA_DIR / 'work-2020-true-1.json'\n",
    "\n",
    "with work_file.open('r', encoding='UTF-8') as infile:\n",
    "    work_json = json.load(infile)\n",
    "print(len(work_json['results']))\n",
    "\n",
    "print(work_json['results'][0].keys())\n",
    "for r in work_json['results']:\n",
    "    clear_output(wait=True)\n",
    "    print(r['id'])\n",
    "    matched_authors = [x for x in r['authorships'] if any(i for i in x['institutions'] if i['id'] in SAMPLE_INSTITUTIONS)]\n",
    "    for auth in matched_authors:\n",
    "        print(auth['author']['id'], auth['author']['display_name'], [i['display_name'] for i in auth['institutions']])\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc971551",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_INSTITUTIONS = (\n",
    "    'https://openalex.org/I27837315',\n",
    "    'https://openalex.org/I4210114445',\n",
    "    'https://openalex.org/I4210151884'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ecc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_ids = set()\n",
    "\n",
    "for work_file in WORKS_DATA_DIR.glob(\"work-*.json\"):\n",
    "    clear_output(wait=True)\n",
    "    print(work_file)\n",
    "    with work_file.open('r', encoding='UTF-8') as infile:\n",
    "        work_json = json.load(infile)\n",
    "    for r in work_json['results']:\n",
    "        print(r['id'])\n",
    "        matched_authors = [x for x in r['authorships'] if any(i for i in x['institutions'] if i.get('id') in SAMPLE_INSTITUTIONS)]\n",
    "        for auth in matched_authors:\n",
    "            author_id = auth['author'].get('id')\n",
    "            if author_id and author_id not in author_ids:\n",
    "                author_ids.add(auth['author']['id'])\n",
    "                print(author_id, auth['author']['display_name'], [i.get('display_name') for i in auth['institutions']])\n",
    "    print('up to', len(author_ids))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e25b6757",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99174\n"
     ]
    }
   ],
   "source": [
    "authors_out = WORKS_DATA_DIR / 'authors_from_works_of_institutions.json'\n",
    "with authors_out.open('w', encoding='UTF-8') as outfile:\n",
    "    json.dump(list(author_ids), outfile)\n",
    "print(len(author_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322a19b",
   "metadata": {},
   "source": [
    "### author_id to authored works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4f53fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTACT_EMAIL = \"matvan@umich.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aca0455b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99174"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with authors_out.open('r', encoding='UTF-8') as infile:\n",
    "    authors = list(json.load(infile))\n",
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0c717e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-31 10:08:50.377025\n",
      "2022-08-31 10:08:51.380112\n"
     ]
    }
   ],
   "source": [
    "def sleep_until_one_second_after(earlier):\n",
    "    target = earlier + dt.timedelta(seconds=1)\n",
    "    while True:\n",
    "        diff = (target - dt.datetime.now()).total_seconds()\n",
    "        if diff <= 0:\n",
    "            return\n",
    "        elif diff <= 0.1:\n",
    "            time.sleep(0.01)\n",
    "        elif diff <= 1.5:\n",
    "            time.sleep(0.1)\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "basically_now = dt.datetime.now()\n",
    "print(basically_now)\n",
    "time.sleep(0.8)\n",
    "sleep_until_one_second_after(basically_now)\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "281fba07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def strip_work(rec):\n",
    "    new_rec = {'id': minimize_id(rec['id'])}\n",
    "    new_rec.update(**{k: rec[k] for k in ('ids', 'publication_year', 'publication_date', 'title', 'type')})\n",
    "    new_rec['author_list'] = [minimize_id(x.get('author', {}).get('id')) for x in rec['authorships']]\n",
    "    new_rec['host_venue_id'] = minimize_id(rec.get('host_venue', {}).get('id'))\n",
    "    new_rec['abstract'] = reconstitute_abstract(rec.get('abstract_inverted_index', {}))\n",
    "    return new_rec\n",
    "\n",
    "# with (AUTHORS_DIR / 'A648449047-001.json').open('r', encoding='UTF-8') as infile:\n",
    "#     test_full_json = json.load(infile)\n",
    "# print(len(test_full_json['results']))\n",
    "\n",
    "# test_item = test_full_json['results'][0]\n",
    "\n",
    "\n",
    "# stripped = strip_work(test_item)\n",
    "# print(len(str(stripped)), len(str(test_item)), '\\n', stripped, '\\n\\n', test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "eae3c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openalex.org/A2297362609 A2297362609\n"
     ]
    }
   ],
   "source": [
    "def minimize_id(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    return s.replace(\"https://openalex.org/\", \"\")\n",
    "\n",
    "test_string = 'https://openalex.org/A2297362609'\n",
    "print(test_string, minimize_id(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "595d51b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like donuts and i like pie\n"
     ]
    }
   ],
   "source": [
    "def reconstitute_abstract(abstract_inverted_index):\n",
    "    if not abstract_inverted_index:\n",
    "        return ''\n",
    "    reverted = dict()\n",
    "    for k, vs in abstract_inverted_index.items():\n",
    "        for v in vs:\n",
    "            reverted[v] = k\n",
    "    abstract = ' '.join(s for i, s in sorted(reverted.items(), key=lambda x: (x[0])))\n",
    "    return abstract\n",
    "\n",
    "# i like donuts and i like pie\n",
    "test_abstract = {'and': [3], 'donuts': [2], 'i': [0, 4], 'like': [1, 5], 'pie': [6]}\n",
    "print(reconstitute_abstract(test_abstract))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92ee345f",
   "metadata": {},
   "source": [
    "## This converted the full raw jsons (17 GB at 38k files) to \n",
    "## the stripped-down versions (a mere 1.9 GB)\n",
    "\n",
    "import os\n",
    "\n",
    "testnum = 0\n",
    "\n",
    "all_raw_json_files = list((DATA_DIR / 'raw_works_by_author').glob(\"*json\"))\n",
    "for i, raw_path in enumerate(all_raw_json_files):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i+1:>7} of {len(all_raw_json_files)}\", raw_path, end=\" -> \")\n",
    "    raw_size = os.path.getsize(str(raw_path))\n",
    "    path = AUTHORS_DIR / raw_path.name\n",
    "    with raw_path.open('r', encoding='UTF-8') as infile:\n",
    "        raw = json.load(infile)\n",
    "\n",
    "    new_result = {}\n",
    "    new_result['meta'] = raw['meta']\n",
    "    new_result['results'] = [strip_work(w) for w in raw['results']]\n",
    "\n",
    "    with path.open('w', encoding='UTF-8') as outfile:\n",
    "        json.dump(new_result, outfile)\n",
    "    new_size = os.path.getsize(str(path))\n",
    "    print(path, f\"{new_size/raw_size:.1%}\")\n",
    "    raw_path.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "09679e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_server_hit = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d9f718de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_exhausted(meta):\n",
    "    return meta['per_page'] * meta['page'] >= meta['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ba00d4ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99173 of 99174 A4274067696 1\n",
      "https://api.openalex.org/works?mailto=matvan@umich.edu&filter=author.id:A4274067696&per_page=200&page=1\n",
      "Going back to OpenAlex after 1.0 seconds\n",
      "2022-09-01 13:03:40.827334\n",
      "{'count': 1, 'db_response_time_ms': 23, 'page': 1, 'per_page': 200}\n",
      "data\\works_by_author\\A4274067696-001.json\n"
     ]
    }
   ],
   "source": [
    "request_timeout = 5\n",
    "per_page = 200\n",
    "\n",
    "for author_n, full_author_id in enumerate(authors):\n",
    "    clear_output(wait=True)\n",
    "    author_id = full_author_id.replace('https://openalex.org/', '')\n",
    "    print(author_n, 'of', len(authors), author_id, i)\n",
    "\n",
    "    base_url = f\"https://api.openalex.org/works?mailto={CONTACT_EMAIL}\"\n",
    "    filtering = f\"filter=author.id:{author_id}\"\n",
    "    \n",
    "    for i in range(1, 1000):\n",
    "        pagination = f\"per_page={per_page}&page={i}\"\n",
    "        url = \"&\".join((base_url, filtering, pagination))\n",
    "        print(url)\n",
    "\n",
    "        filename = f\"{author_id}-{i:0>3}.json\"\n",
    "        path = AUTHORS_DIR / filename\n",
    "        \n",
    "        # we're going to gather and handle the errors later, \n",
    "        # not retry now\n",
    "        error_path = DATA_DIR / f\"ERROR-{filename}\"\n",
    "        if error_path.exists():\n",
    "            continue\n",
    "        \n",
    "        # if the path is there, we can just check to see\n",
    "        # if we need to bother fetching the next\n",
    "        if path.exists():\n",
    "            with path.open('r', encoding='UTF-8') as infile:\n",
    "                existing_json = json.load(infile)\n",
    "            existing_meta = existing_json.get('meta')\n",
    "            if count_exhausted(existing_meta):\n",
    "                print(f'Skipping {author_id} based on {path}')\n",
    "                break\n",
    "                \n",
    "        testnum += 1  \n",
    "        \n",
    "        if path.exists():\n",
    "            print(f\"Skipping {path}\")\n",
    "            continue\n",
    "\n",
    "        sleep_until_one_second_after(last_server_hit)\n",
    "        print(f'Going back to OpenAlex after {round((dt.datetime.now() - last_server_hit).total_seconds(), 1)} seconds')\n",
    "        try:            \n",
    "            last_server_hit = dt.datetime.now()\n",
    "            print(last_server_hit)\n",
    "            raw = httpx.get(url, timeout=request_timeout).json()\n",
    "            last_server_hit = dt.datetime.now()\n",
    "        except Exception as err:\n",
    "            error_path.write_text(\"\\n\".join((full_author_id, url, str(err))))\n",
    "            print(err)\n",
    "            print(error_path)\n",
    "            continue\n",
    "\n",
    "        print(raw['meta'])\n",
    "        \n",
    "        if raw['results']:\n",
    "            print(path)\n",
    "            \n",
    "            new_result = {}\n",
    "            new_result['meta'] = raw['meta']\n",
    "            new_result['results'] = [strip_work(w) for w in raw['results']]\n",
    "            \n",
    "            with path.open('w', encoding='UTF-8') as outfile:\n",
    "                json.dump(new_result, outfile)\n",
    "               \n",
    "        # This (or an earlier) page has exhausted the record count of this author\n",
    "        if count_exhausted(raw['meta']):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c454fec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://openalex.org/A2403083729\\nhttps://api.openalex.org/works?mailto=matvan@umich.edu&filter=author.id:A2403083729&per_page=200&page=1\\nhi'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\".join((full_author_id, url, str('hi')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a851be",
   "metadata": {},
   "source": [
    "copying from above, sorry, but now we're going to hit the errored files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f1d3f4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://openalex.org/A2669589972',\n",
       " 'https://openalex.org/A1978526575',\n",
       " 'https://openalex.org/A1985405156',\n",
       " 'https://openalex.org/A2098295374',\n",
       " 'https://openalex.org/A1673559269',\n",
       " 'https://openalex.org/A2912214977',\n",
       " 'https://openalex.org/A2893840941',\n",
       " 'https://openalex.org/A3140563759',\n",
       " 'https://openalex.org/A2151249208',\n",
       " 'https://openalex.org/A3107835262']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "errored_authors = []\n",
    "for p in (DATA_DIR / '2022-08-31-errors').glob('ERROR*.json'):\n",
    "    author_id = p.name.split('-')[1]\n",
    "    full_author_id = f\"https://openalex.org/{author_id}\"\n",
    "#     print(p, author_id, full_author_id)\n",
    "    errored_authors.append(full_author_id)\n",
    "errored_authors = list(set(errored_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f988ed08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(errored_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6796d5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 of 284 A2628693548 15\n",
      "https://api.openalex.org/works?mailto=matvan@umich.edu&filter=author.id:A2628693548&per_page=20&page=1\n"
     ]
    }
   ],
   "source": [
    "# give lots of time for these\n",
    "request_timeout = 120\n",
    "# and smaller chunks\n",
    "per_page = 20\n",
    "\n",
    "for author_n, full_author_id in enumerate(list(set(errored_authors))):\n",
    "    clear_output(wait=True)\n",
    "    author_id = full_author_id.replace('https://openalex.org/', '')\n",
    "    print(author_n, 'of', len(set(errored_authors)), author_id, i)\n",
    "\n",
    "    base_url = f\"https://api.openalex.org/works?mailto={CONTACT_EMAIL}\"\n",
    "    filtering = f\"filter=author.id:{author_id}\"\n",
    "    \n",
    "    for i in range(1, 1000):\n",
    "        pagination = f\"per_page={per_page}&page={i}\"\n",
    "        url = \"&\".join((base_url, filtering, pagination))\n",
    "        print(url)\n",
    "\n",
    "        filename = f\"{author_id}-{i:0>3}.json\"\n",
    "        path = AUTHORS_DIR / filename\n",
    "        \n",
    "        # this is the error handling version!\n",
    "        error_path = DATA_DIR / f\"ERROR-{filename}\"\n",
    "        #         if error_path.exists():\n",
    "        #             continue\n",
    "        \n",
    "        # we're going to retry & overwrite them all.\n",
    "        # there's 664 errored files\n",
    "        #         # if the path is there, we can just check to see\n",
    "        #         # if we need to bother fetching the next\n",
    "        #         if path.exists():\n",
    "        #             with path.open('r', encoding='UTF-8') as infile:\n",
    "        #                 existing_json = json.load(infile)\n",
    "        #             existing_meta = existing_json.get('meta')\n",
    "        #             if count_exhausted(existing_meta):\n",
    "        #                 print(f'Skipping {author_id} based on {path}')\n",
    "        #                 break\n",
    "                \n",
    "        \n",
    "        #         if path.exists():\n",
    "        #             print(f\"Skipping {path}\")\n",
    "        #             continue\n",
    "\n",
    "        sleep_until_one_second_after(last_server_hit)\n",
    "        print(f'Going back to OpenAlex after {round((dt.datetime.now() - last_server_hit).total_seconds(), 1)} seconds')\n",
    "        try:            \n",
    "            last_server_hit = dt.datetime.now()\n",
    "            print(last_server_hit)\n",
    "            raw = httpx.get(url, timeout=request_timeout).json()\n",
    "            last_server_hit = dt.datetime.now()\n",
    "        except Exception as err:\n",
    "            error_path.write_text(\"\\n\".join((full_author_id, url, str(err))))\n",
    "            print(err)\n",
    "            print(error_path)\n",
    "            continue\n",
    "\n",
    "        print(raw['meta'])\n",
    "        \n",
    "        if raw['results']:\n",
    "            print(path)\n",
    "            \n",
    "            new_result = {}\n",
    "            new_result['meta'] = raw['meta']\n",
    "            new_result['results'] = [strip_work(w) for w in raw['results']]\n",
    "            \n",
    "            with path.open('w', encoding='UTF-8') as outfile:\n",
    "                json.dump(new_result, outfile)\n",
    "               \n",
    "        # This (or an earlier) page has exhausted the record count of this author\n",
    "        if count_exhausted(raw['meta']):\n",
    "            break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2a6c805",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
